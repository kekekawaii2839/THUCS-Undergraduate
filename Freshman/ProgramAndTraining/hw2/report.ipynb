{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT数据\n",
    "\n",
    "以ChatGPT自动构造为主, 通过prompt给出例子的方式, 针对一本书一次性生成多个问题, 之后再针对每条问题生成对应答案.\n",
    "\n",
    "这样的好处是避免token过长, 并且能很好地提高回答质量与准确性.\n",
    "\n",
    "总问答共计近2k条, 覆盖了《天龙八部》《射雕英雄传》《神雕侠侣》三本书.\n",
    "\n",
    "## 1. 生成问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import jsonlines\n",
    "\n",
    "# titles = ['书剑恩仇录', '侠客行', '倚天屠龙记', '天龙八部', '射雕英雄传', '白马啸西风', '碧血剑', '神雕侠侣', '笑傲江湖', '越女剑', '连城诀', '雪山飞狐', '飞狐外传', '鸳鸯刀', '鹿鼎记']\n",
    "titles = ['天龙八部', '射雕英雄传', '神雕侠侣']\n",
    "\n",
    "for title in titles:\n",
    "    user_content = f\"\"\"你好！你能帮我生成60个关于《{title}》的问题吗？有以下四个方面及其例子供你参考：\n",
    "    1. 人物介绍\n",
    "        请你介绍一下《{title}》的主角。\n",
    "    2. 情节\n",
    "        请你讲一下{title}中有名的比武的情节。\n",
    "    3. 人物关系\n",
    "        小龙女和杨过之间是什么关系？他们之间发生了什么故事？\n",
    "    4. 其他\n",
    "        《{title}》讲了什么故事？\n",
    "\n",
    "    请你指明问题对应的书名，按照以下格式生成，并且不要输出额外的任何内容：\n",
    "    00. xxx\n",
    "    01. xxx\n",
    "    02. xxx\n",
    "    03. xxx\n",
    "    ...\n",
    "    58. xxx\n",
    "    59. xxx\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一位研究金庸武侠小说的专家。\"},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "    response = ''\n",
    "    while response == '':\n",
    "        try:\n",
    "            result = openai.Completion.create(messages, engine=\"gpt-3.5-turbo\", temperature=0.9)\n",
    "            response = result['choices'][0]['message']['content']\n",
    "            print(response)\n",
    "        except Exception as e:\n",
    "            print(result)\n",
    "            response = ''\n",
    "\n",
    "    # parse response\n",
    "    questions = response.split('\\n')\n",
    "    questions = [q[4:] for q in questions if q != '']\n",
    "\n",
    "    # write to jsonl\n",
    "    with jsonlines.open('questions.jsonl', mode='a') as writer:\n",
    "        for q in questions:\n",
    "            writer.write({'Question': q, 'Answer': ''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 生成答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import jsonlines\n",
    "\n",
    "def g(question):\n",
    "    answer = ''\n",
    "    user_content = \"请你回答下面这个问题，除了回答本身不需要输出其他任何内容。\\n问题：\"+question+\"\\n\\n你的回答：\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一位研究金庸武侠小说的专家。\"},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "    while answer == '':\n",
    "        try:\n",
    "            result = openai.Completion.create(messages, engine=\"gpt-3.5-turbo\", temperature=0.8)\n",
    "            response = result['choices'][0]['message']['content']\n",
    "            answer = response\n",
    "        except Exception as e:\n",
    "            print('error:', e)\n",
    "            answer = ''\n",
    "    return answer\n",
    "\n",
    "with open('questions.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        question = data['Question']\n",
    "        print(question)\n",
    "        data['Answer'] = g(question)\n",
    "        print(data['Answer'])\n",
    "\n",
    "        with jsonlines.open('result.jsonl', 'a') as haha:\n",
    "            haha.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了以上方式之外, 我还尝试从其他来源, 如知乎、金庸网等, 收集部分数据.\n",
    "\n",
    "并且还有少量数据是使用Claude2, 截取部分维基百科内容提供给它, 同时生成问题与回答的方式构造的.\n",
    "\n",
    "# 预训练数据\n",
    "\n",
    "预训练数据采用金庸的15本小说.\n",
    "\n",
    "# 模型效果分析\n",
    "\n",
    "以下分析将采用SFT测试集的全部数据+少量手动构造问题进行评估.\n",
    "\n",
    "## rougeL计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tiktoken\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GLMConfig, MiniGLM\n",
    "from rouge_chinese import Rouge\n",
    "import jieba\n",
    "import time\n",
    "\n",
    "out_dir = 'out'\n",
    "num_samples = 1 # number of samples to draw\n",
    "max_new_tokens = 512 # number of tokens generated in each sample\n",
    "temperature = 1 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1234\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
    "exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "# -----------------------------------------------------------------------------\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "# init from a model saved in a specific directory\n",
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "config = GLMConfig(**checkpoint['model_args'])\n",
    "model = MiniGLM(config)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "stop_token = tiktoken.get_encoding('cl100k_base').encode('<|endoftext|>', allowed_special='all')[0]\n",
    "\n",
    "def generate(prompt):\n",
    "    global model\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l, errors='replace')\n",
    "\n",
    "    if not prompt.endswith('<|endoftext|>'):\n",
    "        prompt += '<|endoftext|>'\n",
    "\n",
    "    outputs = []\n",
    "    # encode the beginning of the prompt\n",
    "    if prompt.startswith('FILE:'):\n",
    "        with open(prompt[5:], 'r', encoding='utf-8') as f:\n",
    "            prompts = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        for prompt in prompts:\n",
    "            prompt_ids = encode(prompt)\n",
    "            x = (torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "            # run generation\n",
    "            with torch.no_grad():\n",
    "                with ctx:\n",
    "                    for k in range(num_samples):\n",
    "                        y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k, stop_token=stop_token)\n",
    "                        print(\"Prompt:\", prompt)\n",
    "                        output = decode(y[0].tolist())\n",
    "                        outputs.append(output)\n",
    "    else:\n",
    "        prompt_ids = encode(prompt)\n",
    "        x = (torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "        # run generation\n",
    "        with torch.no_grad():\n",
    "            with ctx:\n",
    "                for k in range(num_samples):\n",
    "                    y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k, stop_token=stop_token)\n",
    "                    output = decode(y[0].tolist())\n",
    "                    outputs.append(output)\n",
    "    try:\n",
    "        result = outputs[0].split('<|endoftext|>')[1]\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print('error:', e)\n",
    "        return outputs\n",
    "\n",
    "encoder = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "val_data = np.memmap('qa_val.bin', dtype=np.uint32, mode='r')\n",
    "\n",
    "scorer = Rouge()\n",
    "jieba.initialize()\n",
    "scores = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, len(val_data), 512):\n",
    "    qa_token = val_data[i:i + 512]\n",
    "    qa = encoder.decode(qa_token)\n",
    "    \n",
    "    question = qa.split('<|endoftext|>')[0]\n",
    "    answer = qa.split('<|endoftext|>')[1]\n",
    "    generated_answer = generate(question)\n",
    "    print(question)\n",
    "    print(answer)\n",
    "    print(generated_answer)\n",
    "    score = scorer.get_scores(' '.join(jieba.cut(answer)), ' '.join(jieba.cut(generated_answer)))[0]['rouge-l']['f']\n",
    "    print(score)\n",
    "    scores.append(score)\n",
    "    print('-'*30)\n",
    "\n",
    "print('time:', time.time() - start_time)\n",
    "print('total num:', len(scores))\n",
    "print('average score:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果:\n",
    "\n",
    "time: 86.22463631629944 (此时间计入了计算rougeL的耗时)\n",
    "\n",
    "total num: 174\n",
    "\n",
    "average score: 0.2775983422370355\n",
    "\n",
    "当然, rougeL并不能完全代表模型的效果, 因为它没有考虑模型回答的语义匹配度, 因此我还采用了sentence embedding的方式进行评估.\n",
    "\n",
    "## sentence embedding计算\n",
    "```python\n",
    "import finetuner\n",
    "def cos_sim(data):\n",
    "    model = finetuner.build_model('jinaai/jina-embedding-l-en-v1')\n",
    "    embeddings = finetuner.encode(\n",
    "        model=model,\n",
    "        data=data\n",
    "    )\n",
    "    return finetuner.cos_sim(embeddings[0], embeddings[1])\n",
    "```\n",
    "结果:\n",
    "\n",
    "average score: 0.8239782\n",
    "\n",
    "这个结果更符合语义, 也更接近模型的真实表现.\n",
    "\n",
    "## 手动问题测试\n",
    "\n",
    "问：黄蓉是谁？\n",
    "\n",
    "答：黄蓉是郭靖的妻子，聪明机智，善于应对各种复杂局面。她武功高强，精通桃花岛武学和丐帮武功。在与郭靖相识后，两人共同经历了许多艰难困苦考验后，最终靠了她的智慧和勇气。\n",
    "\n",
    "问：小龙女是哪一派的？\n",
    "\n",
    "答：小龙女是《神雕侠侣》中的女主角，她是古墓派的传人，从小生活在古墓中，与外界隔绝。她的性格冷静。小龙女练就的古墓派绝技，使她在小龙女的教导下也逐渐学为华山派的掌门人。\n",
    "\n",
    "问：《天龙八部》的主角都有谁？\n",
    "\n",
    "答：《天龙八部》中有许多主角，的主角包括郭靖、黄蓉、杨康、穆念慈、欧阳锋、洪七公、丘处机等。这些人物各自有着独特的性格和经历，共同构成了这部经典武侠小说家族品的精彩故事情节。\n",
    "\n",
    "以上问题为随机抽取, 而且不在训练集中, 可以看到尽管由于模型参数量限制, 回答并不完美, 但模型仍能生成比较相关与正确的回答.\n",
    "\n",
    "# 模型优化\n",
    "\n",
    "## 推理优化\n",
    "修改model.generate()函数如下:\n",
    "```python\n",
    "@torch.no_grad()\n",
    "def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, stop_token=None):\n",
    "    \"\"\"\n",
    "    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "    Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits, _ = self(idx_cond)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        # stop if <|endoftext|>\n",
    "        if idx_next.item() == stop_token:\n",
    "            break\n",
    "    return idx\n",
    "```\n",
    "修改后的推理速度大幅提升, 能够保证在max_new_tokens=512的情况下, 1s内(使用NVIDIA GeForce RTX 3060 Laptop)生成回答.\n",
    "\n",
    "## 调整编码\n",
    "在预训练阶段, 我发现模型经常会生成乱码, 如'�'.\n",
    "\n",
    "经过搜索, 我了解到gpt-2是字节对编码, 对于中文字符会对应1~3个token, 2个token占大部分.\n",
    "\n",
    "这样会导致对数据集切分的时候, 将原本属于一个字符的tokens切分到不同的句子中, 从而导致模型生成乱码.\n",
    "\n",
    "我的改进方案是更换编码, 采用了tiktoken库在GitHub上readme中使用的cl100k_base, 经过测试发现此编码可以将中文字符编码为单个token, 从而避免了上述问题.\n",
    "\n",
    "代码如下:\n",
    "```python\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
